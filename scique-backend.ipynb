{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastapi nest-asyncio pyngrok uvicorn tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate bitsandbytes pydantic","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n\n\ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nmodel_dir = \"/kaggle/input/llm-se-debertav3-large\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_model = SentenceTransformer(SIM_MODEL, device='cuda')\n# sentence_model.max_seq_length = MAX_LENGTH\n# sentence_model = sentence_model.half() # half precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## below this need to do in every API calls","metadata":{}},{"cell_type":"code","source":"# change the trn.prompt.values to a single prompt (question and every choice is space separated)\n# prompt_embeddings = sentence_model.encode(\n#     trn.prompt.values,\n#     batch_size=BATCH_SIZE,\n#     device=DEVICE,\n#     show_progress_bar=True,\n#     convert_to_tensor=True,\n#     normalize_embeddings=True,\n# )\n# prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n# _ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Get the top 3 pages that are likely to contain the topic of interest\n# search_score, search_index = sentence_index.search(prompt_embeddings, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting relevent titles","metadata":{}},{"cell_type":"code","source":"# # it's better if we keep this in memory\n# df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n#                      columns=['id', 'file'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Get the article and associated file location using the index\n# wikipedia_file_data = []\n\n# for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n#     scr_idx = idx\n#     _df = df.loc[scr_idx].copy()\n#     _df['prompt_id'] = i\n#     wikipedia_file_data.append(_df)\n# wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n# wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n# ## Save memory - delete df since it is no longer necessary\n# # del df\n# _ = gc.collect()\n# # libc.malloc_trim(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Get the full text data\n# wiki_text_data = []\n\n# for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n#     _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n#     _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n#     _df_temp = _df[_df['id'].isin(_id)].copy()\n#     del _df\n#     _ = gc.collect()\n#     libc.malloc_trim(0)\n#     wiki_text_data.append(_df_temp)\n# wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n# _ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Parse documents into sentences\n# processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Get embeddings of the wiki text data\n# wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n#                                     batch_size=BATCH_SIZE,\n#                                     device=DEVICE,\n#                                     show_progress_bar=True,\n#                                     convert_to_tensor=True,\n#                                     normalize_embeddings=True)#.half()\n# wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## change this to create only a single prompt\n\n# ## Combine all answers\n# trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n# ## Search using the prompt and answers to guide the search\n# trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n# question_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## change this as well for a single prompt\n\n# ## Parameter to determine how many relevant sentences to include\n# NUM_SENTENCES_INCLUDE = 5\n\n# ## List containing just Context\n# contexts = []\n\n# for r in tqdm(trn.itertuples(), total=len(trn)):\n\n#     prompt_id = r.Index\n\n#     prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n#     if prompt_indices.shape[0] > 0:\n#         prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n#         prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n#         context = \"\"\n        \n#         ## Get the top matches\n#         ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n#         for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n#             context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n        \n#     contexts.append(context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# model_dir = \"/kaggle/input/llm-se-debertav3-large\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dataclasses import dataclass\n# from typing import Optional, Union\n\n# import torch\n# import numpy as np\n# import pandas as pd\n# from datasets import Dataset\n# from transformers import AutoTokenizer\n# from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n# from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df = pd.read_csv(\"test_context.csv\")\n# test_df.index = list(range(len(test_df)))\n# test_df.id = list(range(len(test_df)))\n# test_df[\"prompt\"] = test_df[\"context\"] + \" #### \" +  test_df[\"prompt\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### didn't use the following yet","metadata":{}},{"cell_type":"code","source":"# test_df['answer'] = 'A'\n# test_ds = Dataset.from_pandas(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(model_dir)\n# model = AutoModelForMultipleChoice.from_pretrained(model_dir)\n# model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# def predictions_to_map_output(predictions):\n#     sorted_answer_indices = np.argsort(-predictions)\n#     top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n#     top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n#     return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n# options = 'ABCDE'\n# indices = list(range(5))\n\n# option_to_index = {option: index for option, index in zip(options, indices)}\n# index_to_option = {index: option for option, index in zip(options, indices)}\n\n# def preprocess(example):\n#     # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n#     # so we'll copy our question 5 times before tokenizing\n#     first_sentence = [example['prompt']] * 5\n#     second_sentence = []\n#     for option in options:\n#         second_sentence.append(example[option])\n#     # Our tokenizer will turn our text into token IDs BERT can understand\n#     tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n#     tokenized_example['label'] = option_to_index[example['answer']]\n#     return tokenized_example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @dataclass\n# class DataCollatorForMultipleChoice:\n#     tokenizer: PreTrainedTokenizerBase\n#     padding: Union[bool, str, PaddingStrategy] = True\n#     max_length: Optional[int] = None\n#     pad_to_multiple_of: Optional[int] = None\n    \n#     def __call__(self, features):\n#         label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n#         labels = [feature.pop(label_name) for feature in features]\n#         batch_size = len(features)\n#         num_choices = len(features[0]['input_ids'])\n#         flattened_features = [\n#             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n#         ]\n#         flattened_features = sum(flattened_features, [])\n        \n#         batch = self.tokenizer.pad(\n#             flattened_features,\n#             padding=self.padding,\n#             max_length=self.max_length,\n#             pad_to_multiple_of=self.pad_to_multiple_of,\n#             return_tensors='pt',\n#         )\n#         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n#         batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n#         return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/science-exam-trained-model-weights/run_2').cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = Trainer(\n#     model=model,\n#     tokenizer=tokenizer,\n#     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer)\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_predictions = trainer.predict(tokenized_test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.DataFrame({\"id\": np.arange(len(test_df))})\n# submission_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\n\n# submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fast API","metadata":{}},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom pydantic import BaseModel, validator\nfrom fastapi.middleware.cors import CORSMiddleware\n\nimport os\nimport requests\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom tqdm import tqdm\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n\n\nclass MCQ(BaseModel):\n    question: str\n    choices: list[str]\n\n    @validator(\"choices\")\n    def validate_choices_length(cls, choices):\n        min_length = 1  # Minimum allowed length\n        max_length = 5  # Maximum allowed length\n\n        if len(choices) < min_length:\n            raise ValueError(f\"Choices list must have at least {min_length} item(s)\")\n        if len(choices) > max_length:\n            raise ValueError(f\"Choices list can have at most {max_length} items\")\n\n        return choices\n\n    @validator(\"question\")\n    def validate_question_length(cls, question):\n        min_length = 1  # Minimum allowed length\n        max_length = 512  # Maximum allowed length\n\n        if len(question) < min_length:\n            raise ValueError(f\"Question must have at least {min_length} character(s)\")\n        if len(question) > max_length:\n            raise ValueError(f\"Question can have at most {max_length} characters\")\n\n        return question\n\n\napp = FastAPI(\n    title=\"Scique API\",\n    description=\"An API to answer any MCQ from wikipedia without context and other questions with context.\",\n    version=\"0.1.0\",\n    docs_url=\"/docs\",\n    redoc_url=\"/redoc\",\n)\n\norigins = [\"http://localhost:8000\", \"localhost:8000\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# eluwa_model: AutoModelForCausalLM | None = None\n# tokenizer: AutoTokenizer | None = None\n\n# sentence_model: SentenceTransformer | None = None\n# sentence_index: faiss.Index | None = None\n\nwiki_parquet: pd.DataFrame | None = None\nsentence_index_address: str| None = None\n\n\n# load model artifacts on startup of the application to reduce latency\n@app.on_event(\"startup\")\nasync def startup_event():\n    global sentence_index_address, wiki_parquet\n\n    wiki_parquet = pd.read_parquet(\n        \"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n        columns=[\"id\", \"file\"],\n    )\n    \n    sentence_index_address = input(\"Enter address to wiki-faiss index search url: \")\n\n\n@app.get(\"/\", tags=[\"root\"])\nasync def root() -> str:\n    return \"Welcome to Scique API!\"\n\n\n@app.put(\"/askLLM/\", tags=[\"askLLM\"])\nasync def ask_mcq(mcq: MCQ | None):\n    global wiki_parquet, sentence_index_address\n\n    response = requests.put(sentence_index_address + \"/search_wiki/\", json=mcq.dict())\n    \n    if response.status_code == 200:\n        print(\"PUT request was successful.\")\n    else:\n        print(f\"PUT request failed with status code {response.status_code}\")\n        \n    data = response.json()\n    \n    search_score, search_index = data[0][0], data[1][0]\n    \n    search_score = [[float(x) for x in search_score]]\n    search_index = [[int(x) for x in search_index]]\n\n    ## Get the article and associated file location using the index\n    wikipedia_file_data = []\n\n    for i, (scr, idx) in tqdm(\n        enumerate(zip(search_score, search_index)), total=len(search_score)\n    ):\n        scr_idx = idx\n        _df = wiki_parquet.loc[scr_idx].copy()\n        _df[\"prompt_id\"] = i\n        wikipedia_file_data.append(_df)\n    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n    wikipedia_file_data = (\n        wikipedia_file_data[[\"id\", \"prompt_id\", \"file\"]]\n        .drop_duplicates()\n        .sort_values([\"file\", \"id\"])\n        .reset_index(drop=True)\n    )\n\n    _ = gc.collect()\n    # libc.malloc_trim(0)\n\n    ## Get the full text data\n    wiki_text_data = []\n\n    for file in tqdm(\n        wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())\n    ):\n        _id = [\n            str(i)\n            for i in wikipedia_file_data[wikipedia_file_data[\"file\"] == file][\n                \"id\"\n            ].tolist()\n        ]\n        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=[\"id\", \"text\"])\n\n        _df_temp = _df[_df[\"id\"].isin(_id)].copy()\n        del _df\n        _ = gc.collect()\n        libc.malloc_trim(0)\n        wiki_text_data.append(_df_temp)\n    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n\n    _ = gc.collect()\n\n    ## Parse documents into sentences\n    processed_wiki_text_data = process_documents(\n        wiki_text_data.text.values, wiki_text_data.id.values\n    )\n    \n    return processed_wiki_text_data\n\n#     ## Get embeddings of the wiki text data\n#     wiki_data_embeddings = model.encode(\n#         processed_wiki_text_data.text,\n#         batch_size=BATCH_SIZE,\n#         device=DEVICE,\n#         show_progress_bar=True,\n#         convert_to_tensor=True,\n#         normalize_embeddings=True,\n#     )  # .half()\n#     wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n\n#     _ = gc.collect()\n\n#     question_embeddings = model.encode(\n#         [question],\n#         batch_size=BATCH_SIZE,\n#         device=DEVICE,\n#         show_progress_bar=True,\n#         convert_to_tensor=True,\n#         normalize_embeddings=True,\n#     )\n#     question_embeddings = question_embeddings.detach().cpu().numpy()\n\n#     ## Parameter to determine how many relevant sentences to include\n#     NUM_SENTENCES_INCLUDE = 5\n\n#     ## List containing just Context\n#     # contexts = []\n\n#     prompt_id = 0\n\n#     prompt_indices = processed_wiki_text_data[\n#         processed_wiki_text_data[\"document_id\"].isin(\n#             wikipedia_file_data[wikipedia_file_data[\"prompt_id\"] == prompt_id][\n#                 \"id\"\n#             ].values\n#         )\n#     ].index.values\n\n#     if prompt_indices.shape[0] > 0:\n#         prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n#         prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n#         context = \"\"\n\n#         ## Get the top matches\n#         ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n#         for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n#             context += (\n#                 processed_wiki_text_data.loc[prompt_indices][\"text\"].iloc[_i] + \" \"\n#             )\n\n#     # contexts.append(context)\n#     prompt = context + \" #### \" + mcq.question","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\n\n# specify a port\nport = 8000\nngrok_tunnel = ngrok.connect(port)\n\nnest_asyncio.apply()\n\n\n# where we can visit our fastAPI app\nprint('Public URL:', ngrok_tunnel.public_url)\n\n# finally run the app\nuvicorn.run(app, port=port)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}